{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "You can use this notebook to get the activations and the Representational Similarity Matrices (RSMs) for mouse visual areas and the pretrained models in response to natural videos. The details of the Representational Similarity Analysis are explained in the paper. \n",
    "\n",
    "You can also use this notebook to run a full comparison between all the brain areas and the pretrained models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get activations and RSMs for Allen data\n",
    "\n",
    "Use the code below to get the activations and Representational Similarity Matrices (RSMs) of mouse visual areas in response to natural videos. get_RSM() downloads the data specified with the arguments from Allen Brain Observatory server, and estimates RSMs and their noise ceilings for the specied population of neurons. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from allenDataAnalysis import *\n",
    "\n",
    "area = 'VISpm'\n",
    "\n",
    "all_RSM_Allen, noise_ceiling_rsm, noise_ceiling_cka, activations = get_RSM(\n",
    "                                                                            CreLine = ['Cux2-CreERT2'], TargetedStruct = [area], \n",
    "                                                                            ImagingDepth = [175,275], StimType = 'natural_movie_one',downsample_rate = 1/(3*5), \n",
    "                                                                            pool_sessions = True\n",
    "                                                                            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get activations and RSMs for ANNs\n",
    "\n",
    "Use the code below for getting the activations and Representational Similarity Matrices (RSMs) of the pretrained models (ResNet-1p and ResNet-2p) in response to natural videos.\n",
    "\n",
    "For ResNet-1p and ResNet-2p, set the backbone variable to 'monkeynet-1p' and 'monkeynet-2p', respectively. Also, use the path to the corresponding checkpoint files for the path argument in get_CPC_RSMs().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from deepModelsAnalysis import *\n",
    "\n",
    "model_type = 'CPC' \n",
    "backbone = 'monkeynet_2p' # 'monkeynet_1p'\n",
    "pretrained = True\n",
    "\n",
    "PATH_CPC_monkeynet_2p = os.path.join(os.getcwd(),\"../Checkpoints/cpc_p2.pth.tar\")\n",
    "PATH_CPC_monkeynet_1p = os.path.join(os.getcwd(),\"../Checkpoints/cpc_p1.pth.tar\")\n",
    "\n",
    "all_RSM_CPC, activations_CPC, model = get_CPC_RSMs(\n",
    "                                                    StimType = 'natural_movies',\n",
    "                                                    backbone = backbone,\n",
    "                                                    pretrained =pretrained,\n",
    "                                                    path=PATH_CPC_monkeynet_2p,\n",
    "                                                    frame_per_block = 5, \n",
    "                                                    ds_rate=3\n",
    "                                                    ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare mouse visual areas and ANNs\n",
    "\n",
    "Use the code below to compare brain areas in mouse visual cortex and pretrained models. \n",
    "\n",
    "'model_type' argument takes a list of tuples with three elements: ('loss','backbone','path_to_checkpoint'). 'loss' can be 'CPC or 'ActionRecog'. If the path to the checkpint is not set, a randomly initialized model will be included in the comparisons.\n",
    "\n",
    "'area' argument takes a list of visual areas that will be compared with the pretrained models. \n",
    "\n",
    "The example below, compares area VISp with a randomly intialized ResNet-1p, ResNet-1p and ResNet-2p pretrained with the action recognition loss, and ResNet-1p and ResNet-2p pretrained with the CPC loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from compare_reps import *\n",
    "\n",
    "PATH_CPC_monkeynet_p2 = os.path.join(os.getcwd(),\"../Checkpoints/cpc_p2.pth.tar\")\n",
    "PATH_CPC_monkeynet_p1 = os.path.join(os.getcwd(),\"../Checkpoints/cpc_p1.pth.tar\")\n",
    "PATH_ActionRecog_monkeynet_p2 = os.path.join(os.getcwd(),\"../Checkpoints/act_recog_p2.pth.tar\")\n",
    "PATH_ActionRecog_monkeynet_p1 = os.path.join(os.getcwd(),\"../Checkpoints/act_recog_p1.pth.tar\")\n",
    "\n",
    "R = compare_reps(model_type = [('CPC','monkeynet_p1'),('ActionRecog','monkeynet_p1',PATH_ActionRecog_monkeynet_p1),('ActionRecog','monkeynet_p2',PATH_ActionRecog_monkeynet_p2),('CPC','monkeynet_p1',PATH_CPC_monkeynet_p1),('CPC','monkeynet_p2',PATH_CPC_monkeynet_p2)], \n",
    "             StimType = 'natural_movies', \n",
    "             area = [('VISp',275,'Cux2-CreERT2')], \n",
    "             path_fig = '../Figures/',\n",
    "             compare_models = True,\n",
    "             plot_hierarchy = False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
